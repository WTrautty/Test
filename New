import pandas as pd
import numpy as np
from scipy.stats import chi2_contingency, spearmanr
import plotly.graph_objects as go
import plotly.express as px

def generate_failure_analysis_tab(df):
    required_cols = ["Revised LEC", "Closure Code 1", "Task Owner", "Completed Date", "Create Date", "PID"]
    if not all(col in df.columns for col in required_cols):
        return "<div id='failureTab' class='main-tab'><h2>Required columns not found.</h2></div>"

    df = df.copy()
    df["Revised LEC"] = df["Revised LEC"].fillna("Unknown").astype(str)
    df["Task Owner"] = df["Task Owner"].fillna("Unknown").astype(str)
    df["Is Fail"] = df["Closure Code 1"].str.lower().str.contains("fail|failed|failure", na=False).astype(int)
    df["Completed Date"] = pd.to_datetime(df["Completed Date"], errors="coerce")
    df["Create Date"] = pd.to_datetime(df["Create Date"], errors="coerce")
    df["Days to Close"] = (df["Completed Date"] - df["Create Date"]).dt.days
    df = df.dropna(subset=["Completed Date", "Create Date"])
    if df.empty:
        return "<div id='failureTab' class='main-tab'><h2>No valid data after date processing.</h2></div>"
    df["Month"] = df["Completed Date"].dt.to_period("M").astype(str)

    # === Merge with SITE_INFO ===
    try:
        site_info = pd.read_excel(SITE_INFO_PATH, sheet_name="USDA Master", engine='openpyxl')
        site_info = site_info[["Lumen Ckt ID", "Time Zone", "MA", "State", "Site Type (Ex lg, lg, med, small)", "Phase", "Complex or Simple EXD?", "PC/PM"]]
        site_info = site_info.rename(columns={"Lumen Ckt ID": "PID"})
        merge_stats = df.merge(site_info, on="PID", how="left", indicator=True)
        unmatched = (merge_stats["_merge"] == "left_only").mean()
        if unmatched > 0.5:
            return f"<div id='failureTab' class='main-tab'><h2>Warning: {unmatched*100:.1f}% of PIDs unmatched in SITE_INFO merge.</h2></div>"
        df = merge_stats.drop(columns=["_merge"])
    except Exception as e:
        return f"<div id='failureTab' class='main-tab'><h2>Error loading SITE_INFO: {e}</h2></div>"

    # === Association Strengths ===
    assoc_x = []
    assoc_y = []
    assoc_text = []
    p_values = []
    num_tests = 0

    def check_contingency_validity(contingency):
        """Check if contingency table meets chi-squared assumptions."""
        _, _, _, expected = chi2_contingency(contingency)
        return (expected >= 5).mean() >= 0.8  # At least 80% of cells should have expected count >= 5

    def annotate_strength(value, p_value, alpha=0.05):
        significance = "significant" if p_value < alpha else "not significant"
        if value >= 0.3:
            return f"Moderate association ({significance})"
        elif value >= 0.1:
            return f"Weak association ({significance})"
        else:
            return f"Negligible association ({significance})"

    # Revised LEC
    lec_contingency = pd.crosstab(df["Revised LEC"], df["Is Fail"])
    if (lec_contingency.sum(axis=1) > 0).sum() > 1 and check_contingency_validity(lec_contingency):
        lec_chi2, lec_p, _, _ = chi2_contingency(lec_contingency)
        lec_n = lec_contingency.sum().sum()
        lec_min_dim = min(lec_contingency.shape) - 1
        lec_cramers_v = np.sqrt(lec_chi2 / (lec_n * lec_min_dim)) if lec_min_dim > 0 else 0
        if (lec_contingency.index == "Unknown").mean() > 0.5:
            assoc_text.append("Warning: Over 50% of Revised LEC values are 'Unknown'")
            lec_cramers_v = 0
        else:
            assoc_x.append("Revised LEC")
            assoc_y.append(lec_cramers_v)
            assoc_text.append(f"Cramér's V = {lec_cramers_v:.3f}<br>p = {lec_p:.3f}<br>({annotate_strength(lec_cramers_v, lec_p)})")
            p_values.append(lec_p)
            num_tests += 1
    else:
        assoc_x.append("Revised LEC")
        assoc_y.append(0)
        assoc_text.append("Insufficient data or invalid contingency table")

    # Task Owner
    owner_contingency = pd.crosstab(df["Task Owner"], df["Is Fail"])
    if (owner_contingency.sum(axis=1) > 0).sum() > 1 and check_contingency_validity(owner_contingency):
        owner_chi2, owner_p, _, _ = chi2_contingency(owner_contingency)
        owner_n = owner_contingency.sum().sum()
        owner_min_dim = min(owner_contingency.shape) - 1
        owner_cramers_v = np.sqrt(owner_chi2 / (owner_n * owner_min_dim)) if owner_min_dim > 0 else 0
        if (owner_contingency.index == "Unknown").mean() > 0.5:
            assoc_text.append("Warning: Over 50% of Task Owner values are 'Unknown'")
            owner_cramers_v = 0
        else:
            assoc_x.append("Task Owner")
            assoc_y.append(owner_cramers_v)
            assoc_text.append(f"Cramér's V = {owner_cramers_v:.3f}<br>p = {owner_p:.3f}<br>({annotate_strength(owner_cramers_v, owner_p)})")
            p_values.append(owner_p)
            num_tests += 1
    else:
        assoc_x.append("Task Owner")
        assoc_y.append(0)
        assoc_text.append("Insufficient data or invalid contingency table")

    # Monthly Volume
    monthly_data = df.groupby("Month").agg(
        Fail_Rate=("Is Fail", "mean"),
        Order_Count=("Is Fail", "count")
    ).reset_index()
    if len(monthly_data["Order_Count"].unique()) > 1 and len(monthly_data) >= 3:
        volume_corr, volume_p = spearmanr(monthly_data["Order_Count"], monthly_data["Fail_Rate"])
        assoc_x.append("Monthly Volume")
        assoc_y.append(abs(volume_corr))  # Use absolute value for display
        assoc_text.append(f"Spearman r = {volume_corr:.3f}<br>p = {volume_p:.3f}<br>({annotate_strength(abs(volume_corr), volume_p)})")
        p_values.append(volume_p)
        num_tests += 1
    else:
        assoc_x.append("Monthly Volume")
        assoc_y.append(0)
        assoc_text.append("Insufficient variability or data points for correlation")

    # Time to Close
    if df["Days to Close"].nunique() > 1 and len(df) >= 3:
        time_corr, time_p = spearmanr(df["Days to Close"], df["Is Fail"])
        assoc_x.append("Time to Close")
        assoc_y.append(abs(time_corr))
        assoc_text.append(f"Spearman r = {time_corr:.3f}<br>p = {time_p:.3f}<br>({annotate_strength(abs(time_corr), time_p)})")
        p_values.append(time_p)
        num_tests += 1
    else:
        assoc_x.append("Time to Close")
        assoc_y.append(0)
        assoc_text.append("Insufficient variability or data points for correlation")

    # SITE_INFO Variables
    extra_cols = ["Time Zone", "MA", "State", "Site Type (Ex lg, lg, med, small)", "Phase", "Complex or Simple EXD?", "PC/PM"]
    for col in extra_cols:
        if col in df.columns and df[col].nunique() > 1:
            contingency = pd.crosstab(df[col].fillna("Unknown"), df["Is Fail"])
            if (contingency.sum(axis=1) > 0).sum() > 1 and check_contingency_validity(contingency):
                chi2, p, _, _ = chi2_contingency(contingency)
                n = contingency.sum().sum()
                k = min(contingency.shape) - 1
                cramers_v = np.sqrt(chi2 / (n * k)) if k > 0 else 0
                if (contingency.index == "Unknown").mean() > 0.5:
                    assoc_text.append(f"Warning: Over 50% of {col} values are 'Unknown'")
                    cramers_v = 0
                else:
                    assoc_x.append(col)
                    assoc_y.append(cramers_v)
                    assoc_text.append(f"Cramér's V = {cramers_v:.3f}<br>p = {p:.3f}<br>({annotate_strength(cramers_v, p)})")
                    p_values.append(p)
                    num_tests += 1
            else:
                assoc_x.append(col)
                assoc_y.append(0)
                assoc_text.append(f"Insufficient data or invalid contingency table for {col}")

    # Apply Bonferroni correction
    alpha = 0.05 / max(num_tests, 1)  # Avoid division by zero
    assoc_text = [
        text.replace("(Moderate association)", f"(Moderate association, {'significant' if p < alpha else 'not significant'})")
              .replace("(Weak association)", f"(Weak association, {'significant' if p < alpha else 'not significant'})")
              .replace("(Negligible association)", f"(Negligible association, {'significant' if p < alpha else 'not significant'})")
        for text, p in zip(assoc_text, p_values + [1] * (len(assoc_text) - len(p_values)))
    ]

    assoc_fig = go.Figure(data=[
        go.Bar(
            x=assoc_x,
            y=assoc_y,
            text=assoc_text,
            textposition="auto",
            marker_color=px.colors.qualitative.Pastel
        )
    ])
    assoc_fig.update_layout(
        title="Factors Associated with Failure Rates",
        yaxis_title="Association Strength",
        yaxis=dict(range=[-0.1, 0.5]),
        height=500
    )

    assoc_desc = f"""
    <p>This chart shows factors associated with task failures, based on statistical measures (Cramér's V for categorical variables, Spearman r for numerical variables).<br>
    <b>Higher values indicate stronger statistical relationships</b>, but do not imply causation.<br>
    <ul>
      <li><b>Revised LEC</b> may show a notable association with failures, depending on the data.</li>
      <li><b>Task Owner</b>, <b>Time to Close</b>, and site details vary in their association strength.</li>
      <li>Significance is adjusted for multiple testing (Bonferroni, α = {alpha:.3f}).</li>
    </ul></p>
    """

    return f"""
    <div id="failureTab" class="main-tab">
      <h2>Failure Analysis</h2>
      {assoc_fig.to_html(full_html=False, include_plotlyjs=True)}
      {assoc_desc}
    </div>
    """
